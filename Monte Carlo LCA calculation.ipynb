{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for calculating and storing as HDF5 Monte Carlo LCA results for an entire DB (aggregated results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import brightway2 as bw\n",
    "import os               # to use \"operating system dependent functionality\"\n",
    "import numpy as np      # \"the fundamental package for scientific computing with Python\"\n",
    "import pandas as pd     # \"high-performance, easy-to-use data structures and data analysis tools\" for Python\n",
    "import csv\n",
    "import stats_arrays\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0c64d1467d22>, line 109)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-0c64d1467d22>\"\u001b[1;36m, line \u001b[1;32m109\u001b[0m\n\u001b[1;33m    try dataset=hdf5_file[dataset_path] :\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from brightway2 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse.linalg import factorized, spsolve\n",
    "from scipy import sparse\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import click\n",
    "import scipy as sp\n",
    "import h5py\n",
    "from bw2calc.matrices import MatrixBuilder\n",
    "from stats_arrays.random import MCRandomNumberGenerator\n",
    "import random\n",
    "\n",
    "##################\n",
    "# HDF5 functions #\n",
    "##################\n",
    "\n",
    "# All those functions work based on LCA objects from Brightway\n",
    "\n",
    "# function to rebuild csr matrix from hdf5 storage\n",
    "\n",
    "def hdf5_to_csr_matrix(hdf5_file,group_full_path):\n",
    "    \n",
    "    # Access hdf5 group of the csr info\n",
    "    group=hdf5_file[group_full_path]\n",
    "    \n",
    "    #Rebuild csr matrix\n",
    "    csr=sp.sparse.csr_matrix((group['data'][:],group['indices'][:],group['indptr'][:]), group.attrs['shape'])\n",
    "    \n",
    "    return csr;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "def hdf5_to_LCA_dict(hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    #Retrieve Keys and Items\n",
    "    keys_0=group.attrs['keys_0']\n",
    "    keys_1_list=group['keys_1'][()].decode('UTF-8') #### Use .decode('UTF-8') to convert keys_1_list items for bytes to str ?\n",
    "    items_list=group['values'][()]\n",
    "    \n",
    "    keys_list=[(keys_0,keys_1) for keys_1 in keys_1_list]\n",
    "    \n",
    "    #Rebuild LCA_dict\n",
    "    LCA_dict={}\n",
    "    \n",
    "    LCA_dict=dict(zip(keys_list,items_list))\n",
    "    \n",
    "    return LCA_dict;\n",
    "\n",
    "\n",
    "#Function to write csr matrix, _dict from LCA objects and any numpy.ndarray\n",
    "\n",
    "def write_LCA_obj_to_HDF5_file(LCA_obj,hdf5_file,group_path):\n",
    "    \n",
    "    dict_names_to_check=['biosphere_dict', 'activity_dict', 'product_dict']\n",
    "    \n",
    "    #If object = A or B matrix\n",
    "    if type(LCA_obj)==sp.sparse.csr.csr_matrix:\n",
    "        #csr_matrix_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "        #### Direct copy of the function because the call to the function does not work... --> Works now!\n",
    "        # Retrieve or create groups and subgroups\n",
    "        group=hdf5_file.require_group(group_path)\n",
    "\n",
    "        # Create datasets containing values of csr matrix\n",
    "        group.create_dataset('data',data=LCA_obj.data,compression=\"gzip\",dtype=np.float32)\n",
    "        group.create_dataset('indptr',data=LCA_obj.indptr,compression=\"gzip\")\n",
    "        group.create_dataset('indices',data=LCA_obj.indices,compression=\"gzip\")\n",
    "\n",
    "        group.attrs['shape']=LCA_obj.shape\n",
    "        ######\n",
    "        \n",
    "        \n",
    "    \n",
    "    #If object = ***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in dict_names_to_check:\n",
    "        LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #store as float32 if type is float64 to save space\n",
    "        if LCA_obj.dtype == np.dtype('float64'):\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\",dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\")\n",
    "            \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "def h5py_dataset_iterator(g, prefix=''):\n",
    "    for key in g.keys():\n",
    "        item = g[key]\n",
    "        path = '{}/{}'.format(prefix, key)\n",
    "        if isinstance(item, h5py.Dataset): # test for dataset\n",
    "            yield (path, item)\n",
    "        elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "            yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "            \n",
    "def append_scalar_to_list_hdf5_dataset(hdf5_file,dataset_path,value):\n",
    "    \n",
    "    try dataset=hdf5_file[dataset_path] :\n",
    "        dataset.resize((dataset.shape[0]+1,))\n",
    "        dataset[-1] = value\n",
    "    else:\n",
    "        hdf5_file.create_dataset(dataset_path,shape=(1,), maxshape=(None,))\n",
    "        dataset=hdf5_file[dataset_path]\n",
    "        dataset[-1] = value\n",
    "        \n",
    "    return;\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Dependant LCI Monte Carlo functions #\n",
    "#######################################\n",
    "\n",
    "\n",
    "#Dependant LCA Monte Carlo for each activity and functional unit defined in functional_units = [{act.key: 1}]\n",
    "def worker_process(project, \n",
    "                   job_id, \n",
    "                   worker_id, \n",
    "                   iterations,\n",
    "                   functional_units,\n",
    "                   collector_functional_unit,\n",
    "                   hdf5_file_MC_LCI_results_path,\n",
    "                   hdf5_file_deterministic_lci_path,\n",
    "                   hdf5_file_MC_LCA_results_aggregated_path,\n",
    "                   hdf5_file_MC_LCA_results_disaggregated_path,\n",
    "                   impact_method_name_list,\n",
    "                   results_disaggregated_or_not):\n",
    "    \n",
    "    #Open the HDF5 files for each worker to write LCA results\n",
    "    hdf5_file_MC_LCI_results=h5py.File(hdf5_file_MC_LCI_results_path,'r')\n",
    "    lci_complete_iterations=int(hdf5_file_MC_LCI_results.attrs['Number of complete iterations'])\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_disaggregated_path,'a')\n",
    "    else:\n",
    "        hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_aggregated_path,'a')\n",
    "    \n",
    "    #Retrieve B matrix for uncertainty LCI 0 \n",
    "    hdf5_file_deterministic_lci=h5py.File(hdf5_file_deterministic_lci_path,'r')\n",
    "    biosphere_matrix_lci0=hdf5_to_csr_matrix(hdf5_file_deterministic_lci,'/biosphere_matrix')\n",
    "    \n",
    "    #Retrieve biosphere_dict and activity_dict which is the same for the entire database\n",
    "    biosphere_dict=hdf5_to_LCA_dict(hdf5_file_MC_LCI_results,'/biosphere_dict')\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        activity_dict=hdf5_to_LCA_dict(hdf5_file_MC_LCI_results,'/activity_dict')\n",
    "    \n",
    "    #Construct the CF useful info (cf_params and cf_rng) for all LCIA methods\n",
    "    impact_method_dict={}\n",
    "    \n",
    "    for impact_method_name in impact_method_name_list:\n",
    "        \n",
    "        method_filepath = Method(impact_method_name).filepath_processed()\n",
    "\n",
    "        cf_params, _, _, characterization_matrix = MatrixBuilder.build(\n",
    "                    method_filepath,\n",
    "                    \"amount\",\n",
    "                    \"flow\",\n",
    "                    \"row\",\n",
    "                    row_dict=biosphere_dict,\n",
    "                    one_d=True,\n",
    "                )\n",
    "\n",
    "        cf_rng = MCRandomNumberGenerator(cf_params, seed=None)\n",
    "        \n",
    "        impact_method_dict[impact_method_name]={}\n",
    "        impact_method_dict[impact_method_name]['cf_params']=cf_params\n",
    "        impact_method_dict[impact_method_name]['cf_rng']=cf_rng\n",
    "        impact_method_dict[impact_method_name]['characterization_matrix_deterministic']=characterization_matrix\n",
    "\n",
    "    \n",
    "    #Retrieve the number of complete iterations (all activities calculated for one iteration)\n",
    "    try:\n",
    "        lca_complete_iterations=int(hdf5_file_MC_LCA_results.attrs['Number of complete iterations'])\n",
    "        print('--Previous number of complete iterations for worker {} is {}'.format(worker_id, lca_complete_iterations))\n",
    "    except:\n",
    "        hdf5_file_MC_LCA_results.attrs['Number of complete iterations']=0\n",
    "        lca_complete_iterations=0\n",
    "    \n",
    "    \n",
    "    inventory_lci0_dict={}\n",
    "       \n",
    "    #LCA iterations\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #Name of the iteration for the storage, starts from 0\n",
    "        lca_iteration_name=lca_complete_iterations\n",
    "        \n",
    "        #Randomly choose an LCI iteration\n",
    "        lci_iteration_name=random.randint(0,lci_complete_iterations-1)\n",
    "        \n",
    "        #Retrieve B matrix for uncertainty LCI 1\n",
    "        biosphere_matrix=hdf5_to_csr_matrix(hdf5_file_MC_LCI_results,'/biosphere_matrix/'+str(lci_iteration_name))      \n",
    "        \n",
    "        #Regenerate the CF matrix for each iteration for all impact methods\n",
    "        characterization_matrix_dict={}\n",
    "        \n",
    "        for impact_method_name in impact_method_dict:\n",
    "            \n",
    "            cf_params=impact_method_dict[impact_method_name]['cf_params']\n",
    "            cf_rng=impact_method_dict[impact_method_name]['cf_rng']\n",
    "            \n",
    "            characterization_matrix = MatrixBuilder.build_diagonal_matrix(cf_params, biosphere_dict,\"row\", \"row\", new_data=cf_rng.next())#For disaggregated results\n",
    "            \n",
    "            #For disaggregated results\n",
    "            if results_disaggregated_or_not == \"disaggregated\":\n",
    "                characterization_matrix_dict[impact_method_name]=characterization_matrix\n",
    "            \n",
    "            #For aggregated results\n",
    "            else:\n",
    "                characterization_matrix_array=np.array(characterization_matrix.sum(1)) #sum sur axe 0 ou 1? Le vecteur est-il dans le bon sens?\n",
    "                characterization_matrix_dict[impact_method_name]=characterization_matrix_array\n",
    "            \n",
    "        #Iterations per activity\n",
    "        for act_index, fu in enumerate(functional_units):\n",
    "\n",
    "            #Creating UUID for each activity\n",
    "            actKey = list(fu.keys())[0][1]\n",
    "                        \n",
    "            #Retrieve supply_array for uncertainty LCI 1\n",
    "            supply_array=hdf5_file_MC_LCI_results['/supply_array/'+actKey+'/'+str(lci_iteration_name)][()]\n",
    "            \n",
    "            #Calculate inventory for uncertainty LCI 1 \n",
    "            #For disaggregated results --> inventory is a matrix\n",
    "            if results_disaggregated_or_not == \"disaggregated\":\n",
    "                count = len(activity_dict)\n",
    "                inventory = biosphere_matrix * sparse.spdiags([supply_array], [0], count, count) #For disaggregated results\n",
    "            \n",
    "            #For aggregated results --> inventory is a vector\n",
    "            else:\n",
    "                #inventory_array=np.array(inventory.sum(1)) #For aggregated results\n",
    "\n",
    "                #check result -->OK\n",
    "                inventory = biosphere_matrix * supply_array \n",
    "            \n",
    "            \n",
    "            #Retrieve the inventory for uncertainty LCI 0 --> calculate it just once\n",
    "            if iteration ==0:\n",
    "                supply_array_lci0=hdf5_file_deterministic_lci['/supply_array/'+actKey][()]\n",
    "                \n",
    "                if results_disaggregated_or_not == \"disaggregated\":\n",
    "                    count = len(activity_dict)\n",
    "                    inventory_lci0 = biosphere_matrix_lci0 * sparse.spdiags([supply_array_lci0], [0], count, count) #For disaggregated results\n",
    "                else:\n",
    "                    inventory_lci0 = biosphere_matrix_lci0 * supply_array_lci0\n",
    "                \n",
    "                inventory_lci0_dict[actKey]=inventory_lci0\n",
    "                \n",
    "            inventory_lci0=inventory_lci0_dict[actKey]\n",
    "            \n",
    "            \n",
    "            #Calculate impact scores for all impact categories and Store impact_score\n",
    "            for impact_method_name in characterization_matrix_dict:\n",
    "                \n",
    "                characterization_matrix=characterization_matrix_dict[impact_method_name]\n",
    "                characterization_matrix_lcia0=impact_method_dict[impact_method_name]['characterization_matrix_deterministic']\n",
    "                \n",
    "                impact_score_path='/Uncertainty LCI 1 LCIA 1/'+actKey+'/'+str(impact_method_name)\n",
    "                impact_score_lcia0_path='/Uncertainty LCI 1 LCIA 0/'+actKey+'/'+str(impact_method_name)\n",
    "                impact_score_lci0_lcia1_path='/Uncertainty LCI 0 LCIA 1/'+actKey+'/'+str(impact_method_name)\n",
    "                \n",
    "                #For disaggregated results --> impact_score is a matrix of impact scores \n",
    "                #disaggregated per direct contributing activity and direct contributing elementary flows\n",
    "                if results_disaggregated_or_not == \"disaggregated\":\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 1\n",
    "                    impact_score= characterization_matrix * inventory\n",
    "                    impact_score_path=impact_score_path+'/'+str(lca_iteration_name)\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score,hdf5_file_MC_LCA_results,impact_score_path)\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 0\n",
    "                    impact_score_lcia0= characterization_matrix_lcia0 * inventory\n",
    "                    impact_score_lcia0_path=impact_score_lcia0_path+'/'+str(lca_iteration_name)\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score_lcia0,hdf5_file_MC_LCA_results,impact_score_lcia0_path)\n",
    "                    \n",
    "                    #Uncertainty LCI 0 LCIA 1\n",
    "                    impact_score_lci0_lcia1= characterization_matrix * inventory_lci0\n",
    "                    impact_score_lci0_lcia1_path=impact_score_lci0_lcia1_path+'/'+str(lca_iteration_name)\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score_lci0_lcia1,hdf5_file_MC_LCA_results,impact_score_lci0_lcia1_path)\n",
    "                    \n",
    "                    #store info on activities and EF?\n",
    "                    \n",
    "                #For aggregated results --> impact_score is a scalar, store as vector for each impact method    \n",
    "                else:\n",
    "                    #Uncertainty LCI 1 LCIA 1\n",
    "                    characterization_matrix=np.reshape(characterization_matrix,characterization_matrix.shape[0])\n",
    "                    impact_score= np.dot(characterization_matrix, inventory)\n",
    "                    append_scalar_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,impact_score_path,impact_score)\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 0\n",
    "                    characterization_matrix_lcia0=np.array(characterization_matrix_lcia0.sum(1))\n",
    "                    characterization_matrix_lcia0=np.reshape(characterization_matrix_lcia0,characterization_matrix_lcia0.shape[0])\n",
    "                    impact_score_lcia0= np.dot(characterization_matrix_lcia0, inventory)\n",
    "                    append_scalar_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,impact_score_lcia0_path,impact_score_lcia0)\n",
    "                    \n",
    "                    #Uncertainty LCI 0 LCIA 1\n",
    "                    impact_score_lci0_lcia1= np.dot(characterization_matrix, inventory_lci0)\n",
    "                    append_scalar_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,impact_score_lci0_lcia1_path,impact_score_lci0_lcia1)\n",
    "                        \n",
    "            \n",
    "        #Store the list of iteration names from LCI\n",
    "        lci_iteration_name_list_path='/lci_iteration_name_list'\n",
    "        append_scalar_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,lci_iteration_name_list_path,lci_iteration_name)\n",
    "        \n",
    "        \n",
    "        #Count the number of complete iterations for LCA results\n",
    "        lca_complete_iterations=lca_iteration_name+1\n",
    "        hdf5_file_MC_LCA_results.attrs['Number of complete iterations']= lca_complete_iterations\n",
    "            \n",
    "                        \n",
    "    hdf5_file_deterministic_lci.close()\n",
    "    hdf5_file_MC_LCI_results.close()\n",
    "    hdf5_file_MC_LCA_results.close()\n",
    "    \n",
    "    return;\n",
    "                        \n",
    "                \n",
    "                \n",
    "        \n",
    "def get_deterministic_inventory(collector_functional_unit, functional_units, hdf5_file_deterministic_lci, job_id):\n",
    "        \n",
    "    #Get metadata for the HDF5 file\n",
    "    DB_name= list(functional_units[0].keys())[0][0]\n",
    "    \n",
    "    #Write useful information to HDF5 file\n",
    "    hdf5_file_deterministic_lci.attrs['Database name']=DB_name\n",
    "    hdf5_file_deterministic_lci.attrs['Creation ID']=job_id\n",
    "    hdf5_file_deterministic_lci.attrs['Description']='HDF5 file containing the deterministic values for B matrix and supply_array for all activities in the database'\n",
    "    \n",
    "    #Retrieve B matrix for uncertainty LCI 0 --> use sacrificial LCA\n",
    "    sacrificial_lca = LCA(collector_functional_unit)\n",
    "    sacrificial_lca.lci()\n",
    "    biosphere_matrix_lci0=sacrificial_lca.biosphere_matrix\n",
    "    \n",
    "    write_LCA_obj_to_HDF5_file(biosphere_matrix_lci0,hdf5_file_deterministic_lci,'/biosphere_matrix')\n",
    "    \n",
    "    #Retrieve the supply_array for uncertainty LCI 0  \n",
    "    for act_index, fu in enumerate(functional_units):\n",
    "        \n",
    "        #Creating UUID for each activity\n",
    "        actKey = list(fu.keys())[0][1]\n",
    "        \n",
    "        lca = LCA(fu)\n",
    "        lca.lci()\n",
    "        supply_array_lci0=lca.supply_array\n",
    "        \n",
    "        write_LCA_obj_to_HDF5_file(supply_array_lci0,hdf5_file_deterministic_lci,'/supply_array/'+actKey)\n",
    "        \n",
    "    return;    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Dependant_LCA_Monte_Carlo_aggregated_results(project, \n",
    "                                                 database, \n",
    "                                                 iterations, \n",
    "                                                 cpus, \n",
    "                                                 hdf5_file_MC_LCI_results_path, \n",
    "                                                 path_for_saving,\n",
    "                                                 impact_method_name_list,\n",
    "                                                 results_disaggregated_or_not):\n",
    "    \n",
    "    projects.set_current(project)\n",
    "    bw2setup()\n",
    "\n",
    "    #Path the write the results\n",
    "    BASE_OUTPUT_DIR = path_for_saving\n",
    "\n",
    "    #ID to identify who and when was the calculation made\n",
    "    now = datetime.datetime.now()\n",
    "    job_id = \"{}_{}-{}-{}_{}h{}\".format(os.environ['COMPUTERNAME'],now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "    #Selection of activities for MC analysis\n",
    "    db = Database(database)\n",
    "    #activities = [activity for activity in db]\n",
    "    act1=db.get('e929619f245df590fee5d72dc979cdd4')\n",
    "    act2=db.get('bdf7116059abfcc6b8b9ade1a641e578')\n",
    "    act3=db.get('c8c815c68836adaf964daaa001a638a3')\n",
    "    activities = [act1,act2,act3]\n",
    "    \n",
    "    #Create objects to pass the functional units = 1 for each activity\n",
    "    functional_units = [ {act.key: 1} for act in activities ]\n",
    "    collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "    \n",
    "    #Create and save all the useful information related to the deterministic version of the database\n",
    "    path_for_saving=BASE_OUTPUT_DIR\n",
    "    hdf5_file_name=\"Deterministic_LCI.hdf5\"\n",
    "    hdf5_file_deterministic_lci_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "    \n",
    "    if os.path.isfile(hdf5_file_deterministic_lci_path)==False:\n",
    "        hdf5_file_deterministic_lci=h5py.File(hdf5_file_deterministic_lci_path,'a')\n",
    "        get_deterministic_inventory(collector_functional_unit, functional_units, hdf5_file_deterministic_lci, job_id)\n",
    "        hdf5_file_deterministic_lci_path.close()\n",
    "    \n",
    "    #Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "    workers = []\n",
    "\n",
    "    for worker_id in range(cpus):\n",
    "        #Create or open the HDF5 file for each worker and write metadata\n",
    "        hdf5_file_name=\"LCA_Dependant_Monte_Carlo_aggregated_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "        hdf5_file_MC_LCA_results_aggregated_path=BASE_OUTPUT_DIR+\"\\\\\"+hdf5_file_name\n",
    "        hdf5_file_name=\"LCA_Dependant_Monte_Carlo_disaggregated_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "        hdf5_file_MC_LCA_results_disaggregated_path=BASE_OUTPUT_DIR+\"\\\\\"+hdf5_file_name\n",
    "        \n",
    "        if results_disaggregated_or_not == \"disaggregated\":\n",
    "            hdf5_file_MC_results=h5py.File(hdf5_file_MC_LCA_results_disaggregated_path,'a')\n",
    "        else:\n",
    "            hdf5_file_MC_results=h5py.File(hdf5_file_MC_LCA_results_aggregated_path,'a')\n",
    "        \n",
    "        hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "        hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "        hdf5_file_MC_results.attrs['Description']='HDF5 file containing all LCA dependant Monte Carlo results per activity/iteration'\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "\n",
    "        # Create child processes that can work apart from parent process\n",
    "        child = mp.Process(target=worker_process, args=(projects.current, \n",
    "                                                        job_id, worker_id, \n",
    "                                                        iterations,\n",
    "                                                        functional_units,\n",
    "                                                        collector_functional_unit,\n",
    "                                                        hdf5_file_MC_LCI_results_path,\n",
    "                                                        hdf5_file_deterministic_lci_path,\n",
    "                                                        hdf5_file_MC_LCA_results_aggregated_path,\n",
    "                                                        hdf5_file_MC_LCA_results_disaggregated_path,\n",
    "                                                        impact_method_name_list,\n",
    "                                                        results_aggregated_or_not))\n",
    "        workers.append(child)\n",
    "        child.start()\n",
    "        \n",
    "    return;\n",
    "      \n",
    "        \n",
    "        \n",
    "#Useful when the code is run from the console to execute the main function\n",
    "#if __name__ == '__main__':\n",
    "#    Dependant_LCA_Monte_Carlo_aggregated_results()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function bw2data.data_store.ProcessedDataStore.filepath_processed>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw.Method.filepath_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ic_name=('IMPACTWorld+ - Endpoint - only with spatial variability - Four param beta integration - update august 15th 2017','Ecosystem Quality','Land transformation, biodiversity, GLO, with uncert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dico={}\n",
    "dico[ic_name]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('IMPACTWorld+ - Endpoint - only with spatial variability - Four param beta integration - update august 15th 2017',\n",
       "  'Ecosystem Quality',\n",
       "  'Land transformation, biodiversity, GLO, with uncert'): 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('IMPACTWorld+ - Endpoint - only with spatial variability - Four param beta integration - update august 15th 2017', 'Ecosystem Quality', 'Land transformation, biodiversity, GLO, with uncert')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Can't find method object Brightway2 Method: IMPACTWorld+ - Endpoint - only with spatial variability - Four param beta integration - update august 15th 2017: Ecosystem Quality: Land transformation, biodiversity, GLO, with uncert",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dd1b70833e38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mget_filepaths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mic_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\bw2calc\\utils.py\u001b[0m in \u001b[0;36mget_filepaths\u001b[1;34m(name, kind)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdata_store\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOBJECT_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Can't find {} object {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_processed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Can't find method object Brightway2 Method: IMPACTWorld+ - Endpoint - only with spatial variability - Four param beta integration - update august 15th 2017: Ecosystem Quality: Land transformation, biodiversity, GLO, with uncert"
     ]
    }
   ],
   "source": [
    "from bw2calc.utils import (\n",
    "    global_index,\n",
    "    clean_databases,\n",
    "    get_filepaths,\n",
    "    load_arrays,\n",
    "    mapping,\n",
    ")\n",
    "\n",
    "get_filepaths(bw.Method(ic_name),\"method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Laure\\\\AppData\\\\Local\\\\pylca\\\\Brightway3\\\\default.c21f969b5f03d33d43e04f8f136e7682\\\\processed\\\\impactworld-endpoint-only-with-spatial-variability-four-param-beta-integration-update-august-15th-2017el.f206e5ea37ed25b38d29df4d1e9d52ee.npy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw.Method(ic_name).filepath_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
