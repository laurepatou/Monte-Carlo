{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code of database_wide_monte_carlo.py with the HDF5 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import brightway2 as bw\n",
    "import os               # to use \"operating system dependent functionality\"\n",
    "import numpy as np      # \"the fundamental package for scientific computing with Python\"\n",
    "import pandas as pd     # \"high-performance, easy-to-use data structures and data analysis tools\" for Python\n",
    "import csv\n",
    "import stats_arrays\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code contains in database_wide_monte_carlo.py with modification for HDF5 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from brightway2 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse.linalg import factorized, spsolve\n",
    "from scipy import sparse\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import click\n",
    "import scipy as sp\n",
    "import h5py\n",
    "\n",
    "\"\"\"\n",
    "Used to generate uncertainty information at the database level.\n",
    "For each iteration:\n",
    "- New values for uncertain parameters of the technosphere (A) and biosphere (B) matrices are generated\n",
    "- Cradle-to-gate LCI results are calculated for all potential output of the LCI database\n",
    "\n",
    "The following is stored in a specified directory: \n",
    "- All values of the A and B matrices\n",
    "- For each functional unit: \n",
    "    - the supply array (aka the scaling vector)\n",
    "    - the life cycle inventory\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "##################\n",
    "# HDF5 functions #\n",
    "##################\n",
    "\n",
    "# All those functions work based on LCA objects from Brightway\n",
    "\n",
    "# function create a group containing all the information of a csr matrix scipy  \n",
    "\n",
    "def csr_matrix_to_hdf5(csr,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    csr_size=csr.nnz\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('data',data=csr.data,compression=\"gzip\",dtype=np.float32)\n",
    "    group.create_dataset('indptr',data=csr.indptr,compression=\"gzip\")\n",
    "    group.create_dataset('indices',data=csr.indices,compression=\"gzip\")\n",
    "    group.create_dataset('shape',data=csr.shape)\n",
    "\n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "def LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    ###### WARNING : Modify the builder because _dict items are like \n",
    "    #####('ecoinvent 3.3 cutoff', 'c533b046462b6c56a5636ca177347c48'): 35\n",
    "    #### Use .decode('UTF-8') to convert keys_1_list items for bytes to str\n",
    "    \n",
    "    \n",
    "    keys_0=np.string_([key[0] for key in LCA_dict.keys()][0])\n",
    "    keys_1_list=np.string_([key[1] for key in LCA_dict.keys()])\n",
    "    items_list=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys_1',data=keys_1_list,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=items_list,compression=\"gzip\")\n",
    "    group.create_dataset('keys_0',data=keys_0)\n",
    "\n",
    "    return;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _***_dict: _biosphere_dict, _activity_dict, _product_dict\n",
    "def _LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)  \n",
    "    \n",
    "    keys=[key for key in LCA_dict.keys()]\n",
    "    values=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys',data=keys,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=values,compression=\"gzip\")\n",
    "\n",
    "    return;\n",
    "\n",
    "\n",
    "\n",
    "#Function to write csr matrix, _dict from LCA objects and any numpy.ndarray\n",
    "\n",
    "def write_LCA_obj_to_HDF5_file(LCA_obj,hdf5_file,group_path):\n",
    "    \n",
    "    dict_names_to_check=['biosphere_dict', 'activity_dict', 'product_dict']\n",
    "    _dict_names_to_check=['_biosphere_dict', '_activity_dict', '_product_dict']\n",
    "    \n",
    "    #If object = A or B matrix\n",
    "    if type(LCA_obj)==sp.sparse.csr.csr_matrix:\n",
    "        #csr_matrix_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "        #### Direct copy of the function because the call to the function does not work... --> Works now!\n",
    "        # Retrieve or create groups and subgroups\n",
    "        group=hdf5_file.require_group(group_path)\n",
    "\n",
    "        # Create datasets containing values of csr matrix\n",
    "        group.create_dataset('data',data=LCA_obj.data,compression=\"gzip\",dtype=np.float32)\n",
    "        group.create_dataset('indptr',data=LCA_obj.indptr,compression=\"gzip\")\n",
    "        group.create_dataset('indices',data=LCA_obj.indices,compression=\"gzip\")\n",
    "        group.create_dataset('shape',data=LCA_obj.shape)\n",
    "\n",
    "        ######\n",
    "        \n",
    "        \n",
    "    #If object = _***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in _dict_names_to_check:\n",
    "        _LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)    \n",
    "        \n",
    "    \n",
    "    #If object = ***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in dict_names_to_check:\n",
    "        LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #store as float32 if type is float64 to save space\n",
    "        if LCA_obj.dtype == np.dtype('float64'):\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\",dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\")\n",
    "            \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "def h5py_dataset_iterator(g, prefix=''):\n",
    "    for key in g.keys():\n",
    "        item = g[key]\n",
    "        path = '{}/{}'.format(prefix, key)\n",
    "        if isinstance(item, h5py.Dataset): # test for dataset\n",
    "            yield (path, item)\n",
    "        elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "            yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Dependant LCI Monte Carlo functions #\n",
    "#######################################\n",
    "\n",
    "\n",
    "#Change for DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin)?\n",
    "class DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin):\n",
    "    pass\n",
    "\n",
    "#Clean the HDF5 file with LCI MC results for not complete iterations (i.e. not all activities calculated for the last iteration)\n",
    "def clean_hdf5_file_MC_results(hdf5_file_MC_results,worker_id):\n",
    "    \n",
    "    complete_iterations=int(hdf5_file_MC_results.attrs['Number of complete iterations'])\n",
    "    \n",
    "    #Clean techno and bio matrix for not complete iterations\n",
    "    techno_group=hdf5_file_MC_results['/technosphere_matrix']\n",
    "    bio_group=hdf5_file_MC_results['/biosphere_matrix']\n",
    "    techno_iterations=len(techno_group)\n",
    "    bio_iterations=len(bio_group)\n",
    "\n",
    "    print('--Number of iterations for A and B retrieved for worker {}'.format(worker_id))\n",
    "\n",
    "    if techno_iterations!=complete_iterations:\n",
    "        iteration_name_to_delete=techno_iterations-1\n",
    "        del techno_group[str(iteration_name_to_delete)]\n",
    "        #print('--Incomplete iterations for A removed for worker {}'.format(worker_id))\n",
    "\n",
    "    if bio_iterations!=complete_iterations:\n",
    "        iteration_name_to_delete=bio_iterations-1\n",
    "        del bio_group[str(iteration_name_to_delete)]\n",
    "        #print('--Incomplete iterations for B removed for worker {}'.format(worker_id))\n",
    "\n",
    "    #Clean supply arrays for not complete iterations\n",
    "    supply_array_group=hdf5_file_MC_results['/supply_array']\n",
    "\n",
    "    print('--Number of iterations for supply_array retrieved for worker {}'.format(worker_id))\n",
    "\n",
    "    for act in supply_array_group:\n",
    "        supply_act_iterations=len(supply_array_group[act])\n",
    "        if supply_act_iterations!=complete_iterations:\n",
    "            iteration_name_to_delete=supply_act_iterations-1\n",
    "            del supply_array_group[act][str(iteration_name_to_delete)]\n",
    "            #print('--Incomplete iterations for supply_array removed for worker {} for activity {}'.format(worker_id,act))\n",
    "            \n",
    "    return;\n",
    "\n",
    "#Create a file that gather all MC results and Useful info in one file\n",
    "def gathering_MC_results_in_one_hdf5_file(path_for_saving):\n",
    "    \n",
    "    \n",
    "    #Create the gathering file\n",
    "    hdf5_file_all_MC_results_path=os.path.join(path_for_saving,'LCI_Dependant_Monte_Carlo_results_ALL.hdf5')\n",
    "    hdf5_file_all_MC_results=h5py.File(hdf5_file_all_MC_results_path,'w-')\n",
    "    \n",
    "    #Retrieve child file paths\n",
    "    child_hdf5_file_paths = [os.path.join(path_for_saving,fn) for fn in next(os.walk(path_for_saving))[2] if '.hdf5' in fn]\n",
    "    child_MC_results_paths=[path for path in child_hdf5_file_paths if 'LCI_Dependant_Monte_Carlo_results_worker' in path]\n",
    "    child_DB_info_paths=[path for path in child_hdf5_file_paths if 'Useful_info_per_DB' in path]\n",
    "    \n",
    "    #Gathering MC results\n",
    "    complete_iterations=0\n",
    "    \n",
    "    for child_file_path in child_MC_results_paths:\n",
    "                                               \n",
    "        worker_id=child_file_path.rsplit('LCI_Dependant_Monte_Carlo_results_worker', 1)[1]\n",
    "        \n",
    "        start_1 = time.time()\n",
    "        \n",
    "        #Clean incomplete iterations before gathering \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r+')\n",
    "        clean_hdf5_file_MC_results(child_hdf5_file,worker_id)\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "        end_1 = time.time()\n",
    "        print(\"Cleaning in {} secondes for entire DB for worker {}\".format(end_1 - start_1,worker_id)) \n",
    "\n",
    "        start_2 = time.time()\n",
    "        \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r')\n",
    "    \n",
    "        for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):\n",
    "            \n",
    "            #For supply array\n",
    "            if 'supply_array' in child_dataset_path:\n",
    "            \n",
    "                #Create similar path for the master file\n",
    "                child_iteration_name=child_dataset_path.rsplit('/', 1)[1]\n",
    "                master_iteration_name=str(int(child_iteration_name)+complete_iterations)\n",
    "                master_dataset_path='{}/{}'.format(child_dataset_path.rsplit('/', 1)[0],master_iteration_name)\n",
    "\n",
    "                #Link child data to master file\n",
    "                hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "             \n",
    "            #For A and B matrix\n",
    "            else:\n",
    "                #Create similar path for the master file\n",
    "                child_iteration_name=child_dataset_path.rsplit('/', 2)[1]\n",
    "                master_iteration_name=str(int(child_iteration_name)+complete_iterations)\n",
    "                master_dataset_path='{}/{}/{}'.format(child_dataset_path.rsplit('/', 2)[0],master_iteration_name,child_dataset_path.rsplit('/', 2)[2])\n",
    "\n",
    "                #Link child data to master file\n",
    "                hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "\n",
    "        complete_iterations=complete_iterations+int(child_hdf5_file.attrs['Number of complete iterations'])\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "        end_2 = time.time()\n",
    "        print(\"Gathering information in {} secondes for entire DB for {} iterations\".format(end_2 - start_2,complete_iterations)) \n",
    "\n",
    "        \n",
    "    hdf5_file_all_MC_results.attrs['Number of complete iterations']=complete_iterations\n",
    "    \n",
    "    #Useful info\n",
    "    for child_file_path in child_DB_info_paths:\n",
    "        \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r')\n",
    "    \n",
    "        for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):\n",
    "            \n",
    "            #Create the master path\n",
    "            master_dataset_path=child_dataset_path\n",
    "            \n",
    "            #Link child data to master file\n",
    "            hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "        \n",
    "        db_name=child_hdf5_file.attrs['Database name']\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "    hdf5_file_all_MC_results.attrs['Database name']=db_name\n",
    "    \n",
    "    hdf5_file_all_MC_results.close()\n",
    "    \n",
    "        \n",
    "    return;\n",
    "   \n",
    "\n",
    "#Dependant LCI Monte Carlo for each activity and functional unit defined in functional_units = [{act.key: FU}]\n",
    "def worker_process(project, job_id, worker_id, functional_units, iterations,path_for_saving):\n",
    "    \n",
    "    #Open the HDF5 file for each worker\n",
    "    hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker{}.hdf5\".format(str(worker_id))\n",
    "    hdf5_file_MC_results_path=\"{}\\\\{}\".format(path_for_saving,hdf5_file_name)\n",
    "        \n",
    "    hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Retrieve the number of complete iterations (all activities calculated for one iteration)\n",
    "        try:\n",
    "            complete_iterations=int(hdf5_file_MC_results.attrs['Number of complete iterations'])\n",
    "            print('--Previous number of complete iterations for worker {} is {}'.format(worker_id, complete_iterations))\n",
    "        except:\n",
    "            hdf5_file_MC_results.attrs['Number of complete iterations']=0\n",
    "            complete_iterations=0\n",
    "\n",
    "        #Clean the HDF5 files for not complete iterations if needed\n",
    "        if complete_iterations>0:\n",
    "            clean_hdf5_file_MC_results(hdf5_file_MC_results,worker_id)\n",
    "\n",
    "        projects.set_current(project)\n",
    "\n",
    "        #Creating the LCA object --> set fix_dictionaries=False as not useful here?\n",
    "        lca = DirectSolvingMonteCarloLCA(demand = functional_units[0])\n",
    "        lca.load_data()\n",
    "\n",
    "        start_iteration = 0\n",
    "        end_iteration = 0\n",
    "\n",
    "        #Create and save objects per iteration --> Per iteration: A=0.49MB, B=0.34MB, creation time for both=0.35sec \n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            #Name of the iteration for the storage, starts from 0\n",
    "            iteration_name=complete_iterations\n",
    "\n",
    "            print('--Starting job for worker {}, iteration {}, stored as {}, previous complete iteration in {} min'.format(worker_id, iteration,iteration_name,(end_iteration-start_iteration)/60))\n",
    "\n",
    "            start_iteration = time.time()\n",
    "\n",
    "            #start_1 = time.time()\n",
    "            #Creating A and B matrix\n",
    "            lca.rebuild_technosphere_matrix(lca.tech_rng.next())\n",
    "            lca.rebuild_biosphere_matrix(lca.bio_rng.next())\n",
    "\n",
    "            #Saving A and B to HDF5 file\n",
    "            group_path_techno='/technosphere_matrix/{}'.format(str(iteration_name))\n",
    "            group_path_bio='/biosphere_matrix/{}'.format(str(iteration_name))\n",
    "            write_LCA_obj_to_HDF5_file(lca.technosphere_matrix,hdf5_file_MC_results,group_path_techno)\n",
    "            write_LCA_obj_to_HDF5_file(lca.biosphere_matrix,hdf5_file_MC_results,group_path_bio)\n",
    "            hdf5_file_MC_results[group_path_techno].attrs['Creation ID']=job_id\n",
    "            hdf5_file_MC_results[group_path_bio].attrs['Creation ID']=job_id\n",
    "\n",
    "            #Size_A_MB=(hdf5_file_MC_results[group_path_techno+'/data'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indptr'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indices'].id.get_storage_size())/1000000\n",
    "            #Size_B_MB=(hdf5_file_MC_results[group_path_bio+'/data'].id.get_storage_size()+hdf5_file_MC_results[group_path_bio+'/indptr'].id.get_storage_size()+hdf5_file_MC_results[group_path_bio+'/indices'].id.get_storage_size())/1000000\n",
    "\n",
    "\n",
    "\n",
    "            #For calculation\n",
    "            lca.decompose_technosphere()\n",
    "\n",
    "            #end_1 = time.time()\n",
    "            #print(\"Calcul et sauvegarde A et B en {} secondes for iteration {}, and the storage size is A={}MB and B={}MB\".format(end_1 - start_1,iteration,Size_A_MB,Size_B_MB)) \n",
    "\n",
    "            #Create and save objects per activity/iteration --> Per iteration and activity: supply_array=0.04MB, creation time=0.01sec (except for the first activity=0.5sec)\n",
    "            for act_index, fu in enumerate(functional_units):\n",
    "\n",
    "                #start_2 = time.time()\n",
    "\n",
    "                #Creating UUID for each activity\n",
    "                actKey = list(fu.keys())[0][1]\n",
    "\n",
    "                #Create demand_array\n",
    "                lca.build_demand_array(fu)\n",
    "\n",
    "                #Create supply_array\n",
    "                lca.supply_array = lca.solve_linear_system()\n",
    "\n",
    "                #Save supply_array to HDF5 file\n",
    "                group_path_supply='/supply_array/{}/{}'.format(actKey,str(iteration_name))\n",
    "                write_LCA_obj_to_HDF5_file(lca.supply_array,hdf5_file_MC_results,group_path_supply)\n",
    "                hdf5_file_MC_results[group_path_supply].attrs['Creation ID']=job_id\n",
    "\n",
    "                #end_2 = time.time()\n",
    "                #print(\"Calcul et sauvegarde s en {} secondes for iteration {} and activity {}, and the storage size is s={}MB\".format(end_2 - start_2,iteration,actKey,hdf5_file_MC_results[group_path_supply].id.get_storage_size()/1000000)) \n",
    "\n",
    "            #Count the number of complete iterations\n",
    "            complete_iterations=iteration_name+1\n",
    "            hdf5_file_MC_results.attrs['Number of complete iterations']= complete_iterations\n",
    "\n",
    "            end_iteration = time.time()\n",
    "\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "        \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        \n",
    "        hdf5_file_MC_results.flush()\n",
    "        hdf5_file_MC_results.close()\n",
    "        \n",
    "        print(\"Process interrupted\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    return;\n",
    "\n",
    "#TEST OK    \n",
    "def get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities):\n",
    "\n",
    "    # Sacrificial LCA to extract relevant information (demand of 1 for all activities)\n",
    "    # Done on the \"collector\" functional unit to ensure that all activities and \n",
    "    # exchanges are covered in the common dicts (only relevant if some activities \n",
    "    # link to other upstream databases\n",
    "    sacrificial_lca = LCA(collector_functional_unit)\n",
    "    sacrificial_lca.lci()\n",
    "    \n",
    "    #Get data to store \n",
    "    product_dict=sacrificial_lca.product_dict\n",
    "    _product_dict=sacrificial_lca._product_dict\n",
    "    biosphere_dict=sacrificial_lca.biosphere_dict\n",
    "    _biosphere_dict=sacrificial_lca._biosphere_dict\n",
    "    activity_dict=sacrificial_lca.activity_dict\n",
    "    _activity_dict=sacrificial_lca._activity_dict\n",
    "    tech_params=sacrificial_lca.tech_params\n",
    "    bio_params=sacrificial_lca.bio_params\n",
    "    activities_keys=np.string_([act.key[1] for act in activities])\n",
    "    #rev_activity_dict, rev_product_dict, rev_bio_dict = sacrificial_lca.reverse_dict()  --> really needed?\n",
    "    \n",
    "    #Get metadata for the HDF5 file\n",
    "    DB_name=activities[0].key[0]\n",
    "    \n",
    "    #Write useful information to HDF5 file\n",
    "    hdf5_file_useful_info_per_DB.attrs['Database name']=DB_name\n",
    "    hdf5_file_useful_info_per_DB.attrs['Creation ID']=job_id\n",
    "    hdf5_file_useful_info_per_DB.attrs['Description']='HDF5 file containing all useful information related to the database in order to use dependant Monte Carlo results'\n",
    "    \n",
    "    write_LCA_obj_to_HDF5_file(product_dict,hdf5_file_useful_info_per_DB,'/product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_product_dict,hdf5_file_useful_info_per_DB,'/_product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(biosphere_dict,hdf5_file_useful_info_per_DB,'/biosphere_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_biosphere_dict,hdf5_file_useful_info_per_DB,'/_biosphere_dict')\n",
    "    write_LCA_obj_to_HDF5_file(activity_dict,hdf5_file_useful_info_per_DB,'/activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_activity_dict,hdf5_file_useful_info_per_DB,'/_activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(tech_params,hdf5_file_useful_info_per_DB,'/tech_params')\n",
    "    write_LCA_obj_to_HDF5_file(bio_params,hdf5_file_useful_info_per_DB,'/bio_params')\n",
    "    write_LCA_obj_to_HDF5_file(activities_keys,hdf5_file_useful_info_per_DB,'/activities_keys')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_activity_dict,hdf5_file_useful_info_per_DB,'/rev_activity_dict')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_product_dict,hdf5_file_useful_info_per_DB,'/rev_product_dict')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_bio_dict,hdf5_file_useful_info_per_DB,'/rev_bio_dict')\n",
    "\n",
    "    return None;\n",
    "\n",
    "\n",
    "#Useful when the code is run from the console to pass arguments to the main function\n",
    "#@click.command()\n",
    "#@click.option('--project', default='default', help='Brightway2 project name', type=str)\n",
    "#@click.option('--database', help='Database name', type=str)\n",
    "#@click.option('--iterations', default=1000, help='Number of Monte Carlo iterations', type=int)\n",
    "#@click.option('--cpus', default=mp.cpu_count(), help='Number of used CPU cores', type=int)\n",
    "#@click.option('--output_dir', help='Output directory path', type=str)\n",
    "\n",
    "\n",
    "\n",
    "#Create and save useful information during Dependant LCI MC : database objects (_dict, activities, _params, reverse_dict), iteration objects (_sample, i.e. A and B _matrix), act/iteration objects (supply_array)    \n",
    "def Dependant_LCI_Monte_Carlo_results(project, database, iterations, cpus, output_dir):\n",
    "    \n",
    "    projects.set_current(project)\n",
    "    bw2setup()\n",
    "\n",
    "    #Path the write the results\n",
    "    BASE_OUTPUT_DIR = output_dir\n",
    "\n",
    "    #ID to identify who and when was the calculation made\n",
    "    now = datetime.datetime.now()\n",
    "    job_id = \"{}_{}-{}-{}_{}h{}\".format(os.environ['COMPUTERNAME'],now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "    #Selection of activities for MC analysis\n",
    "    db = Database(database)\n",
    "    activities = [activity for activity in db]\n",
    "    #act1=db.get('e929619f245df590fee5d72dc979cdd4')\n",
    "    #act2=db.get('bdf7116059abfcc6b8b9ade1a641e578')\n",
    "    #act3=db.get('c8c815c68836adaf964daaa001a638a3')\n",
    "    #activities = [act1,act2,act3]\n",
    "\n",
    "    #Create objects to pass the functional units = 1 for each activity\n",
    "    functional_units = [ {act.key: 1} for act in activities ]\n",
    "    collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "\n",
    "    #Create or open the HDF5 file for useful information storage per DB\n",
    "    path_for_saving=BASE_OUTPUT_DIR\n",
    "    hdf5_file_name=\"Useful_info_per_DB.hdf5\"\n",
    "    hdf5_file_useful_info_per_DB_path=\"{}\\\\{}\".format(path_for_saving,hdf5_file_name)\n",
    "    \n",
    "    if os.path.isfile(hdf5_file_useful_info_per_DB_path)==False:\n",
    "\n",
    "        hdf5_file_useful_info_per_DB=h5py.File(hdf5_file_useful_info_per_DB_path,'a')\n",
    "\n",
    "        #Create and save all the useful information related to the database only\n",
    "        get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities)\n",
    "\n",
    "        hdf5_file_useful_info_per_DB.close()\n",
    "\n",
    "    #Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "    workers = []\n",
    "\n",
    "    for worker_id in range(cpus):\n",
    "        #Create or open the HDF5 file for each worker and write metadata\n",
    "        hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "        hdf5_file_MC_results_path=\"{}\\\\{}\".format(BASE_OUTPUT_DIR,hdf5_file_name)\n",
    "\n",
    "        hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "\n",
    "        hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "        hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "        hdf5_file_MC_results.attrs['Description']='HDF5 file containing all dependant Monte Carlo results per iteration (A and B matrix) and per activity/iteration (supply_array)'\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "\n",
    "        # Create child processes that can work apart from parent process\n",
    "        child = mp.Process(target=worker_process, args=(projects.current, job_id, worker_id, functional_units, iterations,path_for_saving))\n",
    "        workers.append(child)\n",
    "        child.start()\n",
    "        \n",
    "    return;\n",
    "      \n",
    "#Commande to launch it from the console : python database_wide_monte_carlo_hdf5_storage.py        \n",
    "        \n",
    "#Useful when the code is run from the console to execute the main function\n",
    "#if __name__ == '__main__':\n",
    "#    Dependant_LCI_Monte_Carlo_results(project=\"iw_integration\",\n",
    "#                                      database=\"ecoinvent 3.3 cutoff\",\n",
    "#                                      iterations=27,\n",
    "#                                      cpus=4,\n",
    "#                                      output_dir=\"E:\\\\Brightway calculation\\\\ecoinvent 3.3 cutoff\\\\Dependant LCI Monte Carlo - reduce 100 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to run it in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activate Brighway2\n",
    "- cd C:\\Users\\Laure\\Brightway - Notebooks and code\\database_wide_monte_carlo\n",
    "- python database_wide_monte_carlo_hdf5_storage.py --project \"iw_integration\" --database \"ecoinvent 3.3 cutoff\" --iterations 10 --cpus 4 --output_dir \"D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_for_saving=\"D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo - test 2\"\n",
    "hdf5_file_name=\"Useful_info_per_DB.hdf5\"\n",
    "hdf5_file_useful_info_per_DB_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "\n",
    "hdf5_file_useful_info_per_DB=h5py.File(hdf5_file_useful_info_per_DB_path,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_file_useful_info_per_DB.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing the gathering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_for_saving=\"E:\\\\Brightway calculation\\\\ecoinvent 3.3 cutoff\\\\Dependant LCI Monte Carlo - reduce iterations for test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Number of iterations for A and B retrieved for worker LCI_Dependant_Monte_Carlo_results_worker0.hdf5\n",
      "--Number of iterations for supply_array retrieved for worker LCI_Dependant_Monte_Carlo_results_worker0.hdf5\n",
      "Cleaning in 0.8617620468139648 secondes for entire DB for worker LCI_Dependant_Monte_Carlo_results_worker0.hdf5\n",
      "Gathering information in 78.98435592651367 secondes for entire DB for 2 iterations\n",
      "--Number of iterations for A and B retrieved for worker LCI_Dependant_Monte_Carlo_results_worker1.hdf5\n",
      "--Number of iterations for supply_array retrieved for worker LCI_Dependant_Monte_Carlo_results_worker1.hdf5\n",
      "Cleaning in 84.88901543617249 secondes for entire DB for worker LCI_Dependant_Monte_Carlo_results_worker1.hdf5\n",
      "Gathering information in 125.7565348148346 secondes for entire DB for 4 iterations\n",
      "--Number of iterations for A and B retrieved for worker LCI_Dependant_Monte_Carlo_results_worker2.hdf5\n",
      "--Number of iterations for supply_array retrieved for worker LCI_Dependant_Monte_Carlo_results_worker2.hdf5\n",
      "Cleaning in 98.42241144180298 secondes for entire DB for worker LCI_Dependant_Monte_Carlo_results_worker2.hdf5\n",
      "Gathering information in 138.04752826690674 secondes for entire DB for 6 iterations\n",
      "--Number of iterations for A and B retrieved for worker LCI_Dependant_Monte_Carlo_results_worker3.hdf5\n",
      "--Number of iterations for supply_array retrieved for worker LCI_Dependant_Monte_Carlo_results_worker3.hdf5\n",
      "Cleaning in 110.82924056053162 secondes for entire DB for worker LCI_Dependant_Monte_Carlo_results_worker3.hdf5\n",
      "Gathering information in 137.78371596336365 secondes for entire DB for 8 iterations\n"
     ]
    }
   ],
   "source": [
    "gathering_MC_results_in_one_hdf5_file(path_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_file_name='LCI_Dependant_Monte_Carlo_results_ALL.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_file=h5py.File(path_with_MC_results+'\\\\'+ALL_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "child_hdf5_file_paths = [os.path.join(path_for_saving,fn) for fn in next(os.walk(path_for_saving))[2] if '.hdf5' in fn]\n",
    "child_MC_results_paths=[path for path in child_hdf5_file_paths if 'LCI_Dependant_Monte_Carlo_results_worker' in path]\n",
    "child_DB_info_paths=[path for path in child_hdf5_file_paths if 'Useful_info_per_DB' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_ALL.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker0.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker1.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker2.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker3.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\Useful_info_per_DB.hdf5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_hdf5_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker0.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker1.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker2.hdf5',\n",
       " 'D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\LCI_Dependant_Monte_Carlo_results_worker3.hdf5']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_MC_results_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCI Monte Carlo\\\\Useful_info_per_DB.hdf5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_DB_info_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weird_dataset=ALL_file['/biosphere_matrix/58/data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy of Dependant_LCI_Monte_Carlo to test it on three activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_path_dir=os.getcwd()\n",
    "path_for_saving=current_path_dir+\"\\\\Saved objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project='iw_integration'\n",
    "database='ecoinvent 3.3 cutoff'\n",
    "iterations=5\n",
    "cpus=1\n",
    "output_dir=path_for_saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biosphere database already present!!! No setup is needed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(project, database, iterations, cpus, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing does not work woth Jupyter... To be tested in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the execution time and memory size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projects.set_current(project)\n",
    "\n",
    "#Selection of activities for MC analysis\n",
    "db = Database(database)\n",
    "#activities = [activity for activity in db]\n",
    "act1=db.get('e929619f245df590fee5d72dc979cdd4')\n",
    "act2=db.get('bdf7116059abfcc6b8b9ade1a641e578')\n",
    "act3=db.get('c8c815c68836adaf964daaa001a638a3')\n",
    "activities = [act1,act2,act3]\n",
    "\n",
    "#Create objects to pass the functional units = 1 for each activity\n",
    "functional_units = [ {act.key: 1} for act in activities ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_id=\"fake job id\"\n",
    "worker_id=44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Starting job for worker 44, iteration 0, stored as 0\n",
      "Calcul et sauvegarde A et B en 0.2867012023925781 secondes for iteration 0, and the storage size is A=0.843288MB and B=1.737555MB\n",
      "Calcul et sauvegarde s en 0.6453723907470703 secondes for iteration 0 and activity e929619f245df590fee5d72dc979cdd4, and the storage size is s=0.04357MB\n",
      "Calcul et sauvegarde s en 0.009505748748779297 secondes for iteration 0 and activity bdf7116059abfcc6b8b9ade1a641e578, and the storage size is s=0.04366MB\n",
      "Calcul et sauvegarde s en 0.0075054168701171875 secondes for iteration 0 and activity c8c815c68836adaf964daaa001a638a3, and the storage size is s=0.043575MB\n",
      "--Starting job for worker 44, iteration 1, stored as 1\n",
      "Calcul et sauvegarde A et B en 0.3287336826324463 secondes for iteration 1, and the storage size is A=0.843243MB and B=1.737523MB\n",
      "Calcul et sauvegarde s en 0.37926816940307617 secondes for iteration 1 and activity e929619f245df590fee5d72dc979cdd4, and the storage size is s=0.04361MB\n",
      "Calcul et sauvegarde s en 0.007505178451538086 secondes for iteration 1 and activity bdf7116059abfcc6b8b9ade1a641e578, and the storage size is s=0.043617MB\n",
      "Calcul et sauvegarde s en 0.009007692337036133 secondes for iteration 1 and activity c8c815c68836adaf964daaa001a638a3, and the storage size is s=0.043561MB\n",
      "--Starting job for worker 44, iteration 2, stored as 2\n",
      "Calcul et sauvegarde A et B en 0.30821752548217773 secondes for iteration 2, and the storage size is A=0.843238MB and B=1.737412MB\n",
      "Calcul et sauvegarde s en 0.40378618240356445 secondes for iteration 2 and activity e929619f245df590fee5d72dc979cdd4, and the storage size is s=0.043569MB\n",
      "Calcul et sauvegarde s en 0.01000833511352539 secondes for iteration 2 and activity bdf7116059abfcc6b8b9ade1a641e578, and the storage size is s=0.043639MB\n",
      "Calcul et sauvegarde s en 0.008505821228027344 secondes for iteration 2 and activity c8c815c68836adaf964daaa001a638a3, and the storage size is s=0.043579MB\n",
      "--Starting job for worker 44, iteration 3, stored as 3\n",
      "Calcul et sauvegarde A et B en 0.3137216567993164 secondes for iteration 3, and the storage size is A=0.843339MB and B=1.737553MB\n",
      "Calcul et sauvegarde s en 0.37726712226867676 secondes for iteration 3 and activity e929619f245df590fee5d72dc979cdd4, and the storage size is s=0.043556MB\n",
      "Calcul et sauvegarde s en 0.009507179260253906 secondes for iteration 3 and activity bdf7116059abfcc6b8b9ade1a641e578, and the storage size is s=0.043622MB\n",
      "Calcul et sauvegarde s en 0.008505582809448242 secondes for iteration 3 and activity c8c815c68836adaf964daaa001a638a3, and the storage size is s=0.043584MB\n",
      "--Starting job for worker 44, iteration 4, stored as 4\n",
      "Calcul et sauvegarde A et B en 0.30321598052978516 secondes for iteration 4, and the storage size is A=0.84329MB and B=1.737487MB\n",
      "Calcul et sauvegarde s en 0.3952794075012207 secondes for iteration 4 and activity e929619f245df590fee5d72dc979cdd4, and the storage size is s=0.043596MB\n",
      "Calcul et sauvegarde s en 0.008005619049072266 secondes for iteration 4 and activity bdf7116059abfcc6b8b9ade1a641e578, and the storage size is s=0.043655MB\n",
      "Calcul et sauvegarde s en 0.009006738662719727 secondes for iteration 4 and activity c8c815c68836adaf964daaa001a638a3, and the storage size is s=0.043623MB\n"
     ]
    }
   ],
   "source": [
    "worker_process(project, job_id, worker_id, functional_units, iterations,path_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open the HDF5 file for each worker\n",
    "hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "hdf5_file_MC_results_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "\n",
    "hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "#hdf5_file_MC_results.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Timing to access A and B and s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "A=hdf5_file_MC_results['/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_explore=h5py.File(path_for_saving+\"\\\\\"+'LCI_Dependant_Monte_Carlo_results_worker42.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_explore.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the size of saved objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biosphere_matrix', 'supply_array', 'technosphere_matrix']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in file_to_explore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in file_to_explore['/biosphere_matrix']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'indices', 'indptr']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in file_to_explore['/biosphere_matrix/0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"data\": shape (353369,), type \"<f4\">"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_explore['/biosphere_matrix/0/data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.30574873e-05,   6.00351450e-06,   2.68402516e-07, ...,\n",
       "         1.59020143e-04,   7.18041323e-03,   2.21239566e-03], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_explore['/biosphere_matrix/0/data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27e177dfe67890641d598711b6aa8202',\n",
       " '765bef38d0847348df8807beba76ee9f',\n",
       " 'e49706d6e295e09cd8dbeaef49f8a1f8']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in file_to_explore['/supply_array']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in file_to_explore['/supply_array/27e177dfe67890641d598711b6aa8202']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"8\": shape (13831,), type \"<f4\">"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_explore['/supply_array/27e177dfe67890641d598711b6aa8202/8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.79380162e-07,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "        -0.00000000e+00,  -8.84203301e-12,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_explore['/supply_array/27e177dfe67890641d598711b6aa8202/8'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supply_array=file_to_explore['/supply_array/27e177dfe67890641d598711b6aa8202/8']\n",
    "biosphere_matrix_data=file_to_explore['/biosphere_matrix/0/data']\n",
    "biosphere_matrix_indices=file_to_explore['/biosphere_matrix/0/indices']\n",
    "biosphere_matrix_data=file_to_explore['/biosphere_matrix/0/indptr']\n",
    "techno_matrix_data=file_to_explore['/technosphere_matrix/0/data']\n",
    "techno_matrix_indices=file_to_explore['/technosphere_matrix/0/indices']\n",
    "techno_matrix_data=file_to_explore['/technosphere_matrix/0/indptr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43700"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supply_array.id.get_storage_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490424"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biosphere_matrix_data.id.get_storage_size()+biosphere_matrix_indices.id.get_storage_size()+biosphere_matrix_data.id.get_storage_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349175"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techno_matrix_data.id.get_storage_size()+techno_matrix_indices.id.get_storage_size()+techno_matrix_data.id.get_storage_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
