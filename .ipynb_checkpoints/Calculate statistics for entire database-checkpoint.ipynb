{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate stastistics for an entire database and sectors within the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step : calculating indicators for each {activity|impact method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#MC_results_dict={act:{ic_name:[MC_results]}} as the output of MC_multi_impact_entire_DB()\n",
    "\n",
    "#Stored MC results in HDF5 are np array 1d which size=# iteration\n",
    "#and stored like: Uncertainty LCI 1 LCIA 1/ActKey/impact method name\n",
    "\n",
    "\n",
    "\n",
    "def calculating_endpoint_sum(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics):\n",
    "    \n",
    "    \n",
    "    for uncertainty_level in hdf5_file_MC_LCA_results.items():\n",
    "        \n",
    "        if 'lci_iteration_name_list' not in uncertainty_level[0]:\n",
    "\n",
    "            for act in uncertainty_level[1].items():                    \n",
    "                \n",
    "                for impact_method in act[1].items():\n",
    "                    \n",
    "                    #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                    endpoint_name=impact_method[0].rsplit(',', 1)[0]+')'\n",
    "                    \n",
    "                    #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                    #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "                    \n",
    "                    endpoint_group_path='/'+uncertainty_level[0]+'/'+act[0]+'/'+endpoint_name\n",
    "                    \n",
    "                    contribution_to_add=impact_method[1][()]\n",
    "                    \n",
    "                    try:\n",
    "                        endpoint_sum_dataset=hdf5_file_MC_statistics[endpoint_group_path+'/endpoint_sum']\n",
    "                        endpoint_sum_dataset[...]=endpoint_sum_dataset[()]+contribution_to_add\n",
    "                        \n",
    "                    except:\n",
    "                        hdf5_file_MC_statistics.create_dataset(endpoint_group_path+'/endpoint_sum',data=contribution_to_add)\n",
    "                        \n",
    "    return;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MC_results_dict={act_key:{ic_name:[MC_results]}} as the output of MC_multi_impact_entire_DB()\n",
    "\n",
    "def calculating_endpoint_stats_indicators(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics):\n",
    "  \n",
    "    sum_spear_corr_endpoint={}\n",
    "    \n",
    "    for uncertainty_level in hdf5_file_MC_LCA_results.items():\n",
    "\n",
    "        if 'lci_iteration_name_list' not in uncertainty_level[0]:\n",
    "\n",
    "            for act in uncertainty_level[1].items():                    \n",
    "\n",
    "                for impact_method in act[1].items():\n",
    "\n",
    "                    #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                    endpoint_name=impact_method[0].rsplit(',', 1)[0]+')'\n",
    "                    \n",
    "                    #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                    #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "                    \n",
    "                    endpoint_group_path='/'+uncertainty_level[0]+'/'+act[0]+'/'+endpoint_name\n",
    "                    \n",
    "                    stats_dict={}\n",
    "                    \n",
    "                    #Regular stats\n",
    "                    stats_dict['mean']=np.mean(impact_method[1])\n",
    "                    stats_dict['variance']=np.var(impact_method[1])\n",
    "                    stats_dict['std dev']=np.std(impact_method[1])\n",
    "                    stats_dict['minimum']=min(impact_method[1])\n",
    "                    stats_dict['maximum']=max(impact_method[1])\n",
    "                    stats_dict['2.5th percentile']=np.percentile(impact_method[1],2.5)\n",
    "                    stats_dict['25th percentile']=np.percentile(impact_method[1],25)\n",
    "                    stats_dict['median']=np.percentile(impact_method[1],50)\n",
    "                    stats_dict['75th percentile']=np.percentile(impact_method[1],75)\n",
    "                    stats_dict['97.5th percentile']=np.percentile(impact_method[1],97.5)\n",
    "                    stats_dict['number of iterations']=len(impact_method[1])\n",
    "\n",
    "                    #Stats to measure the dispersion\n",
    "                    stats_dict['MADM']=np.percentile(abs(impact_method[1]-stats_dict['median']),50)\n",
    "                    stats_dict['IQR']=stats_dict['75th percentile']-stats_dict['25th percentile']\n",
    "                    stats_dict['Spread']=stats_dict['maximum']-stats_dict['minimum']\n",
    "                    stats_dict['CI95']=stats_dict['97.5th percentile']-stats_dict['2.5th percentile']\n",
    "                    try:\n",
    "                        stats_dict['Quartile coeff of dispersion']=stats_dict['IQR']/(stats_dict['75th percentile']+stats_dict['25th percentile'])\n",
    "                    except:\n",
    "                        stats_dict['Quartile coeff of dispersion']='NA'\n",
    "                    try:\n",
    "                        stats_dict['CV']=stats_dict['std dev']/stats_dict['mean']\n",
    "                    except:\n",
    "                        stats_dict['CV']='NA'\n",
    "                    try:\n",
    "                        stats_dict['CV modified']=stats_dict['std dev']/np.sqrt((stats_dict['maximum']-stats_dict['mean'])*(stats_dict['mean']-stats_dict['minimum']))\n",
    "                    except:\n",
    "                        stats_dict['CV modified']='NA'\n",
    "                    try:\n",
    "                        stats_dict['CV robust']=stats_dict['MADM']/stats_dict['median']\n",
    "                    except:\n",
    "                        stats_dict['CV robust']='NA'\n",
    "                    try:\n",
    "                        stats_dict['IQR\\spread']=stats_dict['IQR']/(stats_dict['Spread'])\n",
    "                    except:\n",
    "                        stats_dict['IQR\\spread']='NA'\n",
    "                    try:\n",
    "                        stats_dict['IQR\\CI95']=stats_dict['IQR']/stats_dict['CI95']\n",
    "                    except:\n",
    "                        stats_dict['IQR\\CI95']='NA'\n",
    "\n",
    "\n",
    "                    stats_dict['Spearmann rank correlation - coefficient']=stats.spearmanr(impact_method[1],hdf5_file_MC_statistics[endpoint_group_path+'/endpoint_sum'])[0]\n",
    "                    stats_dict['Spearmann rank correlation - pvalue']=stats.spearmanr(impact_method[1],hdf5_file_MC_statistics[endpoint_group_path+'/endpoint_sum'])[1]\n",
    "\n",
    "                    try:\n",
    "                        sum_spear_corr_endpoint[endpoint_name]=sum_spear_corr_endpoint[endpoint_name]+(stats_dict['Spearmann rank correlation - coefficient'])**2\n",
    "\n",
    "                    except:\n",
    "                        sum_spear_corr_endpoint[endpoint_name]=(stats_dict['Spearmann rank correlation - coefficient'])**2\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    #print(str(stats_dict['Spearmann rank correlation - coefficient'])+' with sum '+str(sum_spear_corr_endpoint[endpoint_name]))\n",
    "                    \n",
    "                    \n",
    "                    #Store values\n",
    "                    impact_method_group_path='/'+uncertainty_level[0]+'/'+act[0]+'/'+impact_method[0]\n",
    "                    \n",
    "                    for indicator in stats_dict.keys():\n",
    "                        try:\n",
    "                            hdf5_file_MC_statistics.create_dataset(impact_method_group_path+'/'+indicator,data=stats_dict[indicator])\n",
    "                        except:\n",
    "                            hdf5_file_MC_statistics[impact_method_group_path+'/'+indicator][...]=stats_dict[indicator]\n",
    "                 \n",
    "                \n",
    "                for impact_method in act[1].items():\n",
    "                    \n",
    "                    #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                    endpoint_name=impact_method[0].rsplit(',', 1)[0]+')'\n",
    "                    \n",
    "                    #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                    #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "                    \n",
    "                    impact_method_group_path='/'+uncertainty_level[0]+'/'+act[0]+'/'+impact_method[0]\n",
    "\n",
    "                    #Calculating Contribution To Variance\n",
    "                    stats_dict={}\n",
    "                    stats_dict['Spearmann CTV midpoint to endpoint']=(hdf5_file_MC_statistics[impact_method_group_path+'/Spearmann rank correlation - coefficient'][()])**2/sum_spear_corr_endpoint[endpoint_name]\n",
    "                    \n",
    "                    #Store values\n",
    "                    for indicator in stats_dict.keys():\n",
    "                        try:\n",
    "                            hdf5_file_MC_statistics.create_dataset(impact_method_group_path+'/'+indicator,data=stats_dict[indicator])\n",
    "                        except:\n",
    "                            hdf5_file_MC_statistics[impact_method_group_path+'/'+indicator][...]=stats_dict[indicator]\n",
    "                    \n",
    "                for endpoint_name in sum_spear_corr_endpoint.keys():\n",
    "                    sum_spear_corr_endpoint[endpoint_name]=0\n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "    return;\n",
    "\n",
    "\n",
    "def calculating_endpoint_stats_entire_database_aggregated_MC_results(hdf5_file_MC_LCA_results_path, dir_path_for_saving):\n",
    "    \n",
    "    #Create and/or open the file for MC stats results\n",
    "    hdf5_file_MC_statistics=h5py.File(dir_path_for_saving+'\\\\'+'MC_statistics_aggregated_results.hdf5','w-')\n",
    "    \n",
    "    #Open the MC LCA results file\n",
    "    hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_path,'r')\n",
    "    \n",
    "    #Calculate stats --> only make sense if impact categories in hdf5_file_MC_LCA_results are endpoint per midpoint categories\n",
    "    calculating_endpoint_sum(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics)\n",
    "    calculating_endpoint_stats_indicators(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics)\n",
    "    \n",
    "    #Close hdf5 files\n",
    "    hdf5_file_MC_statistics.close()\n",
    "    hdf5_file_MC_LCA_results.close()\n",
    "    \n",
    "    return;    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\ipykernel_launcher.py:96: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\ipykernel_launcher.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\numpy\\lib\\function_base.py:3162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "C:\\Users\\Laure\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\ipykernel_launcher.py:96: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan with sum nan\n",
      "nan with sum nan\n",
      "nan with sum nan\n",
      "nan with sum nan\n",
      "nan with sum nan\n",
      "nan with sum nan\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n",
      "1.0 with sum 1.0\n"
     ]
    }
   ],
   "source": [
    "hdf5_file_MC_LCA_results_path=\"D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCA Monte Carlo\\\\LCA_Dependant_Monte_Carlo_aggregated_results_ALL.hdf5\"\n",
    "dir_path_for_saving=\"D:\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Test Dependant LCA Monte Carlo\"\n",
    "\n",
    "calculating_endpoint_stats_entire_database_aggregated_MC_results(hdf5_file_MC_LCA_results_path, dir_path_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
