{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the code of database_wide_monte_carlo.py with the HDF5 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import brightway2 as bw\n",
    "import os               # to use \"operating system dependent functionality\"\n",
    "import numpy as np      # \"the fundamental package for scientific computing with Python\"\n",
    "import pandas as pd     # \"high-performance, easy-to-use data structures and data analysis tools\" for Python\n",
    "import csv\n",
    "import stats_arrays\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code contains in database_wide_monte_carlo.py with modification for HDF5 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brightway2 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse.linalg import factorized, spsolve\n",
    "from scipy import sparse\n",
    "import datetime\n",
    "import pickle\n",
    "import click\n",
    "import scipy as sp\n",
    "import h5py\n",
    "\n",
    "\"\"\"\n",
    "Used to generate uncertainty information at the database level.\n",
    "For each iteration:\n",
    "- New values for uncertain parameters of the technosphere (A) and biosphere (B) matrices are generated\n",
    "- Cradle-to-gate LCI results are calculated for all potential output of the LCI database\n",
    "\n",
    "The following is stored in a specified directory: \n",
    "- All values of the A and B matrices\n",
    "- For each functional unit: \n",
    "    - the supply array (aka the scaling vector)\n",
    "    - the life cycle inventory\n",
    "\"\"\" \n",
    "\n",
    "#Change for DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin)?\n",
    "class DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin):\n",
    "    pass\n",
    "\n",
    "#Dependant LCI Monte Carlo for each activity and functional unit defined in functional_units = [{act.key: FU}]\n",
    "def worker_process(project, job_id, worker_id, functional_units, iterations):\n",
    "    \n",
    "    #Open the HDF5 file for each worker\n",
    "    hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "    hdf5_file_MC_results_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "        \n",
    "    hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "    \n",
    "    \n",
    "    ###### Add attributes for metadata!! --> in the main function to write it just once\n",
    "    \n",
    "\n",
    "    projects.set_current(project)\n",
    "    \n",
    "    #Creating the LCA object --> set fix_dictionaries=False as not useful here?\n",
    "    lca = DirectSolvingMonteCarloLCA(demand = functional_units[0])\n",
    "    lca.load_data()\n",
    "    \n",
    "    #Create and save objects per iteration\n",
    "    for iteration in range(iterations):\n",
    "    \n",
    "        print('--Starting job for worker {}, iteration {}'.format(worker_id, iteration))        \n",
    "        \n",
    "        #Creating A and B matrix\n",
    "        lca.rebuild_technosphere_matrix(lca.tech_rng.next())\n",
    "        lca.rebuild_biosphere_matrix(lca.bio_rng.next())\n",
    "        \n",
    "        #Saving A and B to HDF5 file\n",
    "        group_path_techno='/technosphere_matrix/'+str(iteration)\n",
    "        group_path_bio='/biosphere_matrix/'+str(iteration)\n",
    "        write_LCA_obj_to_HDF5_file(lca.technosphere_matrix,hdf5_file_MC_results,group_path_techno)\n",
    "        write_LCA_obj_to_HDF5_file(lca.biosphere_matrix,hdf5_file_MC_results,group_path_bio)\n",
    "        hdf5_file_MC_results[group_path_techno].attrs['Creation ID']=job_id\n",
    "        hdf5_file_MC_results[group_path_bio].attrs['Creation ID']=job_id\n",
    "\n",
    "        #For calculation\n",
    "        lca.decompose_technosphere()\n",
    "        \n",
    "        #Create and save objects per activity/iteration\n",
    "        for act_index, fu in enumerate(functional_units):\n",
    "            \n",
    "            #Creating UUID for each activity\n",
    "            actKey = list(fu.keys())[0][1]\n",
    "            \n",
    "            #Create demand_array\n",
    "            lca.build_demand_array(fu)\n",
    "\n",
    "            #Create supply_array\n",
    "            lca.supply_array = lca.solve_linear_system()\n",
    "            \n",
    "            #Save supply_array to HDF5 file\n",
    "            group_path_supply='/supply_array/'+actKey+'/'+str(iteration)\n",
    "            write_LCA_obj_to_HDF5_file(lca.supply_array,hdf5_file_MC_results,group_path_supply)\n",
    "            hdf5_file_MC_results[group_path_supply].attrs['Creation ID']=job_id\n",
    "            hdf5_file_MC_results['/supply_array/'+actKey].attrs['Number of iterations']=iteration\n",
    "            \n",
    "            \n",
    "    hdf5_file_MC_results.close()\n",
    "        \n",
    "    return;\n",
    "    \n",
    "def get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities):\n",
    "\n",
    "    # Sacrificial LCA to extract relevant information (demand of 1 for all activities)\n",
    "    # Done on the \"collector\" functional unit to ensure that all activities and \n",
    "    # exchanges are covered in the common dicts (only relevant if some activities \n",
    "    # link to other upstream databases\n",
    "    sacrificial_lca = LCA(collector_functional_unit)\n",
    "    sacrificial_lca.lci()\n",
    "    \n",
    "    #Get data to store \n",
    "    product_dict=sacrificial_lca.product_dict\n",
    "    biosphere_dict=sacrificial_lca.biosphere_dict\n",
    "    activity_dict=sacrificial_lca.activity_dict\n",
    "    tech_params=sacrificial_lca.tech_params\n",
    "    bio_params=sacrificial_lca.bio_params\n",
    "    activities_keys=[act.key[1] for act in activities]\n",
    "    rev_activity_dict, rev_product_dict, rev_bio_dict = sacrificial_lca.reverse_dict()\n",
    "    \n",
    "    #Get metadata for the HDF5 file\n",
    "    DB_name=activities[0].key[0]\n",
    "    \n",
    "    #Write useful information to HDF5 file\n",
    "    hdf5_file_useful_info_per_DB.attrs['Database name']=DB_name\n",
    "    hdf5_file_useful_info_per_DB.attrs['Creation ID']=job_id\n",
    "    hdf5_file_useful_info_per_DB.attrs['Description']='HDF5 file containing all useful information related to the database in order to use dependant Monte Carlo results'\n",
    "    \n",
    "    write_LCA_obj_to_HDF5_file(product_dict,hdf5_file_useful_info_per_DB,'/product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(biosphere_dict,hdf5_file_useful_info_per_DB,'/biosphere_dict')\n",
    "    write_LCA_obj_to_HDF5_file(activity_dict,hdf5_file_useful_info_per_DB,'/activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(tech_params,hdf5_file_useful_info_per_DB,'/tech_params')\n",
    "    write_LCA_obj_to_HDF5_file(bio_params,hdf5_file_useful_info_per_DB,'/bio_params')\n",
    "    write_LCA_obj_to_HDF5_file(activities_keys,hdf5_file_useful_info_per_DB,'/activities_keys')\n",
    "    write_LCA_obj_to_HDF5_file(rev_activity_dict,hdf5_file_useful_info_per_DB,'/rev_activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(rev_product_dict,hdf5_file_useful_info_per_DB,'/rev_product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(rev_bio_dict,hdf5_file_useful_info_per_DB,'/rev_bio_dict')\n",
    "\n",
    "    return None;\n",
    "\n",
    "\n",
    "#Create and save useful information during Dependant LCI MC : database objects (_dict, activities, _params, reverse_dict), iteration objects (_sample, i.e. A and B _matrix), act/iteration objects (supply_array)    \n",
    "def Dependant_LCI_Monte_Carlo(project, database, iterations, cpus, output_dir):\n",
    "\n",
    "    projects.set_current(project)\n",
    "    bw2setup()\n",
    "    \n",
    "    #Path the write the results\n",
    "    BASE_OUTPUT_DIR = output_dir\n",
    "    \n",
    "    #ID to identify who and when was the calculation made\n",
    "    now = datetime.datetime.now()\n",
    "    job_id = \"{}_{}-{}-{}_{}h{}\".format(os.environ['COMPUTERNAME'],now.year, now.month, now.day, now.hour, now.minute)\n",
    "    \n",
    "    #Selection of activities for MC analysis\n",
    "    db = Database(database)\n",
    "    activities = [activity for activity in db]\n",
    "    \n",
    "    #Create objects to pass the functional units = 1 for each activity\n",
    "    functional_units = [ {act.key: 1} for act in activities ]\n",
    "    collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "    \n",
    "    #Create or open the HDF5 file for useful information storage per DB\n",
    "    path_for_saving=BASE_OUTPUT_DIR\n",
    "    hdf5_file_name=\"Useful_info_per_DB.hdf5\"\n",
    "    hdf5_file_useful_info_per_DB_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "    \n",
    "    hdf5_file_useful_info_per_DB=h5py.File(hdf5_file_useful_info_per_DB_path,'a')\n",
    "    \n",
    "    #Create and save all the useful information related to the database only\n",
    "    ##os.chdir(BASE_OUTPUT_DIR)\n",
    "    ##job_dir = os.path.join(BASE_OUTPUT_DIR, job_id)\n",
    "    ##os.mkdir(job_dir)\n",
    "    \n",
    "    get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities)\n",
    "    \n",
    "    #Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "    workers = []\n",
    "\n",
    "    for worker_id in range(cpus):\n",
    "        #Create or open the HDF5 file for each worker and write metadata\n",
    "        hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "        hdf5_file_MC_results_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "            \n",
    "        hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "            \n",
    "        hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "        hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "        hdf5_file_MC_results.attrs['Description']='HDF5 file containing all dependant Monte Carlo results per iteration (A and B matrix) and per activity/iteration (supply_array)'\n",
    "            \n",
    "        hdf5_file_MC_results.close()\n",
    "            \n",
    "        # Create child processes that can work apart from parent process\n",
    "        child = mp.Process(target=worker_process, args=(projects.current, job_id, worker_id, functional_units, iterations))\n",
    "        workers.append(child)\n",
    "        child.start()\n",
    "\n",
    "    return;\n",
    "      \n",
    "        \n",
    "        \n",
    "#Useful when the code is run from the console to execute the main function\n",
    "#if __name__ == '__main__':\n",
    "#    Dependant_LCI_Monte_Carlo()\n",
    "\n",
    "#Useful when the code is run from the console to pass arguments to the main function\n",
    "@click.command()\n",
    "@click.option('--project', default='default', help='Brightway2 project name', type=str)\n",
    "@click.option('--database', help='Database name', type=str)\n",
    "@click.option('--iterations', default=1000, help='Number of Monte Carlo iterations', type=int)\n",
    "@click.option('--cpus', default=mp.cpu_count(), help='Number of used CPU cores', type=int)\n",
    "@click.option('--output_dir', help='Output directory path', type=str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################\n",
    "# HDF5 functions #\n",
    "##################\n",
    "\n",
    "# function create a group containing all the information of a csr matrix scipy  \n",
    "\n",
    "def csr_matrix_to_hdf5(csr,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('data',data=csr.data,compression=\"gzip\",dtype=np.float32)\n",
    "    group.create_dataset('indptr',data=csr.indptr,compression=\"gzip\")\n",
    "    group.create_dataset('indices',data=csr.indices,compression=\"gzip\")\n",
    "    group.attrs['shape']=csr.shape\n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "\n",
    "def LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    ###### WARNING : Modify the builder because _dict items are like ('ecoinvent 3.3 cutoff', 'c533b046462b6c56a5636ca177347c48'): 35\n",
    "    \n",
    "    \n",
    "    keys_0=[key[0] for key in LCA_dict.keys()][0]\n",
    "    keys_1_list=[key[1] for key in LCA_dict.keys()]\n",
    "    items_list=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys_1',data=keys_1_list,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=items_list,compression=\"gzip\")\n",
    "    groupe.attrs['keys_0']=keys_0\n",
    "\n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "#Function to write csr matrix, _dict from LCA objects and any numpy.ndarray\n",
    "\n",
    "def write_LCA_obj_to_HDF5_file(LCA_obj,hdf5_file,group_path):\n",
    "    \n",
    "    dict_names_to_check=['biosphere_dict', 'activity_dict', 'product_dict']\n",
    "    \n",
    "    #If object = A or B matrix\n",
    "    if type(LCA_obj)==sp.sparse.csr.csr_matrix:\n",
    "        csr_matrix_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "    \n",
    "    #If object = ***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in dict_names_to_check:\n",
    "        LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #store as float32 if type is float64 to save space\n",
    "        if LCA_obj.dtype == np.dtype('float64'):\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\",dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\")\n",
    "            \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy of Dependant_LCI_Monte_Carlo to test it on three activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_path_dir=os.getcwd()\n",
    "path_for_saving=current_path_dir+\"\\\\Saved objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project='iw_integration'\n",
    "database='ecoinvent 3.3 cutoff'\n",
    "iterations=10\n",
    "cpus=1\n",
    "output_dir=path_for_saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biosphere database already present!!! No setup is needed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9bc99686ff88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m##os.mkdir(job_dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mget_useful_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollector_functional_unit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdf5_file_useful_info_per_DB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7c16306dc94c>\u001b[0m in \u001b[0;36mget_useful_info\u001b[1;34m(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mhdf5_file_useful_info_per_DB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'HDF5 file containing all useful information related to the database in order to use dependant Monte Carlo results'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mwrite_LCA_obj_to_HDF5_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhdf5_file_useful_info_per_DB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/product_dict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[0mwrite_LCA_obj_to_HDF5_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiosphere_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhdf5_file_useful_info_per_DB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/biosphere_dict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mwrite_LCA_obj_to_HDF5_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivity_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhdf5_file_useful_info_per_DB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/activity_dict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7c16306dc94c>\u001b[0m in \u001b[0;36mwrite_LCA_obj_to_HDF5_file\u001b[1;34m(LCA_obj, hdf5_file, group_path)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;31m#If object = ***_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mgroup_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_names_to_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mLCA_dict_to_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLCA_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroup_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7c16306dc94c>\u001b[0m in \u001b[0;36mLCA_dict_to_hdf5\u001b[1;34m(LCA_dict, hdf5_file, group_path)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;31m# Create datasets containing values of csr matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'keys_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys_1_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m     \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitems_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0mgroupe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keys_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mdsid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Brighway2\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mtid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogical\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# Legacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create (C:\\Minonda\\conda-bld\\h5py_1496879986092\\work\\h5py\\h5t.c:18127)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create (C:\\Minonda\\conda-bld\\h5py_1496879986092\\work\\h5py\\h5t.c:17947)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create (C:\\Minonda\\conda-bld\\h5py_1496879986092\\work\\h5py\\h5t.c:17906)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<U32')"
     ]
    }
   ],
   "source": [
    "projects.set_current(project)\n",
    "bw2setup()\n",
    "\n",
    "#Path the write the results\n",
    "BASE_OUTPUT_DIR = output_dir\n",
    "\n",
    "#ID to identify who and when was the calculation made\n",
    "now = datetime.datetime.now()\n",
    "job_id = \"{}_{}-{}-{}_{}h{}\".format(os.environ['COMPUTERNAME'],now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "#Selection of activities for MC analysis\n",
    "db = Database(database)\n",
    "activities = [db.random(),db.random(),db.random()]\n",
    "\n",
    "#Create objects to pass the functional units = 1 for each activity\n",
    "functional_units = [ {act.key: 1} for act in activities ]\n",
    "collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "\n",
    "#Create or open the HDF5 file for useful information storage per DB\n",
    "path_for_saving=BASE_OUTPUT_DIR\n",
    "hdf5_file_name=\"Useful_info_per_DB.hdf5\"\n",
    "hdf5_file_useful_info_per_DB_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "\n",
    "hdf5_file_useful_info_per_DB=h5py.File(hdf5_file_useful_info_per_DB_path,'a')\n",
    "\n",
    "#Create and save all the useful information related to the database only\n",
    "##os.chdir(BASE_OUTPUT_DIR)\n",
    "##job_dir = os.path.join(BASE_OUTPUT_DIR, job_id)\n",
    "##os.mkdir(job_dir)\n",
    "\n",
    "get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities)\n",
    "\n",
    "#Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "workers = []\n",
    "\n",
    "for worker_id in range(cpus):\n",
    "    #Create or open the HDF5 file for each worker and write metadata\n",
    "    hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "    hdf5_file_MC_results_path=path_for_saving+\"\\\\\"+hdf5_file_name\n",
    "\n",
    "    hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "\n",
    "    hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "    hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "    hdf5_file_MC_results.attrs['Description']='HDF5 file containing all dependant Monte Carlo results per iteration (A and B matrix) and per activity/iteration (supply_array)'\n",
    "\n",
    "    hdf5_file_MC_results.close()\n",
    "\n",
    "    # Create child processes that can work apart from parent process\n",
    "    child = mp.Process(target=worker_process, args=(projects.current, job_id, worker_id, functional_units, iterations))\n",
    "    workers.append(child)\n",
    "    child.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
